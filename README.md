# Implementing LIMP model in CHAIC environment

## Question formulation
In the [MuMA-ToM benchmark](https://scai.cs.jhu.edu/projects/MuMA-ToM/), a novel multi-modal multi-agent Theory of Mind method called LIMP (Language model-based Inverse Multi-modal Planning) was proposed. This ToM method integrate the ability of large language models (LLM) and visual language models (VLM) with the Bayesian inference model, and it is compatible with the use of any state-of-art pretrained LLMs without the need of fine tunning.

In the [CHAIC benchmark](https://umass-embodied-agi.github.io/CHAIC/), it suggests an inclusive embodied social intelligence challenge designed to test social perception and cooperation in embodied agents. The challenge requires the helper agent to infer the constrained human agent's intent and make a cooperative plan to help the constrained human agent.

![Constrained Human-AI Cooperation (CHAIC) for benchmarking embodied agents that socially perceive and assist human partners with physical constraints. Left: A human partner is confined to a wheelchair and struggles to move past an obstacle. The helper agent infers the human partnerâ€™s constraints and intents and assists him by removing the obstacle. Right: In a moving house scenario, after observing a human partner fail to lift heavy furniture, the helper agent understands her intents and constraints and assists her in carrying the furniture together.](https://umass-embodied-agi.github.io/CHAIC/figure/teaser_v4.png "CHAIC benckmark")

In our cases, we intend to let LIMP model as a ToM inference model used in the CHAIC environment. At the first stage, we will not implement the actions in the three dimensional simulation environment such as TDW or VirtualHome package, instead we will input the LIMP model with the CHAIC dataset and prompt the model to infer the intent and come up with a high-level plan as text shown in the figure above.

## Implementation details

**Step 1:** 
Using a visual language model to read the video describing the scenario. The video might be generated by TDW package according to the JSON files in the CHAIC dataset. The VLM is supposed to summarize the agent's actions in the scenario in chronological order, preparing the text input for later probability computation.

**Step 2:** 
Converting the text description summarized by the VLM about the scenario into a structured datatype such as Python dictionary. For one agent, its actions and utterances(if defined) can be stored as 
```Python
{"action": action_list, "utterance": utterance_list}
```

**Step 3:** 
Prompting a LLM to generate several hypotheses about the goal of the agent and obstacles faced by the agent. For example, a women agent may has a goal of "carry the sofa to the truck", but is facing an obstacle "it is too heavy". List all of them into a single string:
```Python
f"Given the above scenario description, which of the following statements is MOST likely to be the goal of the person?\nA) {hypothesis1}\nB) {hypothesis2}\nC) {hypothesis3}"
```

**Step 4:**
Then, use the defined Python functions to calculate the probability of each hypothesis given the previous actions and utterances using a LLM. Output the most likely choice from A, B, and C.

Formula for doing the Bayesian inference is as follows,
$$ P(H \mid a_i^{0 \rightarrow T}, u_i^{0 \rightarrow T}, s_0) \\ \propto \prod_{t=1}^T P(a_i^t\mid a_i^{0 \rightarrow t-1},u_i^{0 \rightarrow t-1} ,s_0,H) \times \prod_{t=1}^T P(u_i^t\mid a_i^{0 \rightarrow t-1},u_i^{0 \rightarrow t-1} ,s_0,H)$$ 

where $\small H \in \{A, B, C \}$, $a_i$ and $u_i$ is the actions and utterances of agent i (constrained agent in this case), and $s_0$ is the initial state which can be deduced by LLM. The probability of each time step is given by the log probability generated by the LLM's API call. It is worth noticing that setting ```temperature = 0``` when calling the LLM makes this probability more reliable.

**Step 5:** 
After getting a choice, we can then prompt the LLM to give an oral plan about how it can assist the constrained agent described in the question scenario. For women's example again, the LIMP model may finally conclude that "I may help her to carry together."

## Code layout
Under the folder names LIMP,
```
|__ get_video_description(): Step 1, description generation
|__ parse_text_info(): Step 2, structuring the description
|__ hypotheses_generation(): Step 3, generating question choices
|__ compute_prob_GPT(): Step 4, computing probability
|__ generate_plan(): Step 5, generate a high-level plan
```


## Dataset preparation
In this experiment, we will implement the LIMP model in CHAIC dataset which can be found [here](https://umass-embodied-agi.github.io/CHAIC/).

Setting up TDW in the local device is necessary, follow the steps below to initialize the TDW.
```bash
conda create -n myenv python=3.9
pip3 install tdw
```
Python version is specified here since if tdw is run on Python with higher version (eg. 3.12.0), it may have the error with module "imp". For more information regarding system requirement and test programs, please refer to TDW github [repo](https://github.com/threedworld-mit/tdw).

## Further works
**1.** Preparing the dataset. CHAIC benchmark provides a series of [dataset](https://github.com/UMass-Embodied-AGI/CHAIC/tree/main/dataset), but they are in TDW instructions instead of visual data such as videos. Therefore, generating scenario videos with the use of TDW for VLM input will be necessary. Introduction to TDW can be found through [this link](https://github.com/threedworld-mit/tdw).

**2.** Access for language models. Preparing a language model with both text and video understanding ability, or two separate language models one specifically for text processing (eg. GPT 4o) and another one specifically for video processing (eg. Gemini 2.5 flash).

**3.** Adding procedural generation. Current steps described in Implementation Details are mainly for generating high-level natural language ideas of how out agent can infer the constrained agent's needs and how to help. However, actual motion generation are still required, for example, generating TDW instructions to instruct helper agent to interact with the constrained agent in the virtual environment. There are open-source available codes for procedural generation in both CHAIC and MuMA-ToM benchmarks.

**4.** Potential usage of reinforcement learning. It is worth discussing about the possibilities of adding a state-actions-reward feedback loop for the agent to learn how to help the constrained agent better by rewarding or punishing the agent according to some metrics.
